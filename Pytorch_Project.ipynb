{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qHxOo0o-gdFP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import random_split\n",
        "from pathlib import Path\n",
        "# Importing all the necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307), (0.3081))]\n",
        "    )\n",
        "\n",
        "    trainset = torchvision.datasets.MNIST(\n",
        "        root=data_dir, train=True, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.MNIST(\n",
        "        root=data_dir, train=False, download=True, transform=transform\n",
        "    )\n",
        "    return trainset, testset\n",
        "# defined a function to load the MNIST dataset from the torchvision module and\n",
        "# defined a transform where we are normalizing the single channel images with mean 0.1307 and standard deviation 0.3081\n"
      ],
      "metadata": {
        "id": "u_4gJ4GSg0sU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten=nn.Flatten()\n",
        "    self.linear_relu_stack=nn.Sequential(\n",
        "        nn.Linear(28*28,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.flatten(x)\n",
        "    logits=self.linear_relu_stack(x)\n",
        "    return logits\n",
        "# Have used a simple Neural Network which consists of a flattening layer followed by a sequential stack of fully connected layers with ReLU activation.\n"
      ],
      "metadata": {
        "id": "hkxN1g20iY5r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# have defined a training Neural Network Function and have used help from Pytorch.org\n",
        "def train_MINST(net,data_dir=None):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs]\n",
        "    )\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=int(64), shuffle=True\n",
        "    )\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=int(64), shuffle=True\n",
        "    )\n",
        "\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 100 == 99:  # print every 2000 mini-batches\n",
        "                print( \"[%d, %5d] loss: %.3f\"% (epoch + 1, i + 1, running_loss / epoch_steps) )\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MXbb4Hzji2sv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Xavier Initalization\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n"
      ],
      "metadata": {
        "id": "B7bMUMA-jYq7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=4, shuffle=False\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            #print(predicted)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "ug7eRzjNnZZ5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data(data_dir)\n",
        "    model=Net()\n",
        "    model.apply(init_weights)\n",
        "    device=\"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device=\"cuda:0\"\n",
        "        if torch.cuda.device_count()>1:\n",
        "            model=nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "    epochs=10\n",
        "    train_MINST(model,data_dir)\n",
        "    print(test_accuracy(model,device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sTjZcQZncm0",
        "outputId": "5123f18b-0b09-41c1-844d-b89476c7103b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /content/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 12933670.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/train-images-idx3-ubyte.gz to /content/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /content/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 346050.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/train-labels-idx1-ubyte.gz to /content/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /content/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3195335.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /content/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11708991.25it/s]\n",
            "<ipython-input-7-eb079861a3e6>:4: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/data/MNIST/raw\n",
            "\n",
            "[1,   100] loss: 1.182\n",
            "[1,   200] loss: 0.253\n",
            "[1,   300] loss: 0.133\n",
            "[1,   400] loss: 0.091\n",
            "[1,   500] loss: 0.066\n",
            "[1,   600] loss: 0.052\n",
            "[1,   700] loss: 0.040\n",
            "[2,   100] loss: 0.268\n",
            "[2,   200] loss: 0.118\n",
            "[2,   300] loss: 0.077\n",
            "[2,   400] loss: 0.060\n",
            "[2,   500] loss: 0.043\n",
            "[2,   600] loss: 0.038\n",
            "[2,   700] loss: 0.029\n",
            "[3,   100] loss: 0.208\n",
            "[3,   200] loss: 0.097\n",
            "[3,   300] loss: 0.060\n",
            "[3,   400] loss: 0.045\n",
            "[3,   500] loss: 0.034\n",
            "[3,   600] loss: 0.030\n",
            "[3,   700] loss: 0.025\n",
            "[4,   100] loss: 0.151\n",
            "[4,   200] loss: 0.084\n",
            "[4,   300] loss: 0.051\n",
            "[4,   400] loss: 0.038\n",
            "[4,   500] loss: 0.031\n",
            "[4,   600] loss: 0.025\n",
            "[4,   700] loss: 0.020\n",
            "[5,   100] loss: 0.139\n",
            "[5,   200] loss: 0.068\n",
            "[5,   300] loss: 0.043\n",
            "[5,   400] loss: 0.031\n",
            "[5,   500] loss: 0.025\n",
            "[5,   600] loss: 0.021\n",
            "[5,   700] loss: 0.018\n",
            "[6,   100] loss: 0.115\n",
            "[6,   200] loss: 0.056\n",
            "[6,   300] loss: 0.038\n",
            "[6,   400] loss: 0.032\n",
            "[6,   500] loss: 0.022\n",
            "[6,   600] loss: 0.018\n",
            "[6,   700] loss: 0.016\n",
            "[7,   100] loss: 0.105\n",
            "[7,   200] loss: 0.055\n",
            "[7,   300] loss: 0.034\n",
            "[7,   400] loss: 0.026\n",
            "[7,   500] loss: 0.020\n",
            "[7,   600] loss: 0.015\n",
            "[7,   700] loss: 0.014\n",
            "[8,   100] loss: 0.090\n",
            "[8,   200] loss: 0.050\n",
            "[8,   300] loss: 0.032\n",
            "[8,   400] loss: 0.024\n",
            "[8,   500] loss: 0.017\n",
            "[8,   600] loss: 0.015\n",
            "[8,   700] loss: 0.013\n",
            "[9,   100] loss: 0.079\n",
            "[9,   200] loss: 0.042\n",
            "[9,   300] loss: 0.026\n",
            "[9,   400] loss: 0.020\n",
            "[9,   500] loss: 0.017\n",
            "[9,   600] loss: 0.014\n",
            "[9,   700] loss: 0.013\n",
            "[10,   100] loss: 0.072\n",
            "[10,   200] loss: 0.040\n",
            "[10,   300] loss: 0.023\n",
            "[10,   400] loss: 0.019\n",
            "[10,   500] loss: 0.014\n",
            "[10,   600] loss: 0.013\n",
            "[10,   700] loss: 0.010\n",
            "0.9699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leWgZ-HHnjUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}